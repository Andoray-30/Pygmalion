#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‚è€ƒå›¾ä¸€è‡´æ€§è¯„åˆ† - ä¸Žç”Ÿæˆå›¾è¿›è¡Œå¤šç»´åº¦å¯¹æ¯”
"""
from __future__ import annotations

import cv2
import numpy as np
import torch
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import logging
from typing import Union

logger = logging.getLogger(__name__)


class ReferenceImageMatcher:
    """è¯„ä¼°ç”Ÿæˆå›¾ä¸Žå‚è€ƒå›¾çš„åŒ¹é…åº¦"""
    
    # ç±»çº§åˆ«å…±äº«æ¨¡åž‹ç¼“å­˜ï¼ˆæ‰€æœ‰å®žä¾‹å…±ç”¨ï¼‰
    _shared_model = None
    _shared_processor = None
    _model_device = None

    def __init__(self, device: str | None = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self._model = None
        self._processor = None

    def _lazy_load(self) -> None:
        """æ‡’åŠ è½½æ¨¡åž‹ï¼ˆä½¿ç”¨ç±»çº§åˆ«ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜ä¸”è®¾å¤‡åŒ¹é…
        if (ReferenceImageMatcher._shared_model is None or 
            ReferenceImageMatcher._model_device != self.device):
            logger.info(f"ðŸ”„ é¦–æ¬¡åŠ è½½ CLIP æ¨¡åž‹åˆ° {self.device}...")
            ReferenceImageMatcher._shared_model = CLIPModel.from_pretrained(
                "openai/clip-vit-base-patch32"
            ).to(self.device)
            ReferenceImageMatcher._shared_model.eval()
            ReferenceImageMatcher._shared_processor = CLIPProcessor.from_pretrained(
                "openai/clip-vit-base-patch32"
            )
            ReferenceImageMatcher._model_device = self.device
            logger.info("âœ… CLIP æ¨¡åž‹å·²åŠ è½½å¹¶ç¼“å­˜")
        else:
            logger.info("â™»ï¸ å¤ç”¨å·²ç¼“å­˜çš„ CLIP æ¨¡åž‹")
        
        self._model = ReferenceImageMatcher._shared_model
        self._processor = ReferenceImageMatcher._shared_processor

    @staticmethod
    def _ensure_feature_tensor(output: Union[torch.Tensor, object]) -> torch.Tensor:
        """å…¼å®¹ä¸åŒ transformers ç‰ˆæœ¬çš„è¾“å‡ºç»“æž„ï¼Œç¡®ä¿è¿”å›žå¼ é‡ç‰¹å¾ã€‚"""
        if isinstance(output, torch.Tensor):
            return output
        if hasattr(output, "pooler_output") and output.pooler_output is not None:
            return output.pooler_output
        if hasattr(output, "last_hidden_state") and output.last_hidden_state is not None:
            return output.last_hidden_state.mean(dim=1)
        raise TypeError(f"Unexpected CLIP output type: {type(output)}")

    def evaluate_match(self, reference_image_path: str, generated_image_path: str) -> dict:
        """
        è®¡ç®—ä¸¤å¼ å›¾ç‰‡çš„å¤šç»´åº¦åŒ¹é…åº¦

        Args:
            reference_image_path: å‚è€ƒå›¾è·¯å¾„
            generated_image_path: ç”Ÿæˆå›¾è·¯å¾„

        Returns:
            dict: {
                'style_consistency': 0.82,      # é£Žæ ¼ä¸€è‡´æ€§
                'pose_similarity': 0.75,        # å§¿æ€ç›¸ä¼¼åº¦
                'composition_match': 0.76,      # æž„å›¾ç›¸ä¼¼åº¦
                'character_consistency': 0.70,  # è§’è‰²ä¸€è‡´æ€§
                'overall_reference_match': 0.75 # æ€»ä½“åŒ¹é…åº¦
            }
        """
        try:
            ref_image = Image.open(reference_image_path).convert("RGB")
            gen_image = Image.open(generated_image_path).convert("RGB")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
            return self._default_scores()

        self._lazy_load()

        scores = {}

        # 1. é£Žæ ¼ä¸€è‡´æ€§ï¼ˆCLIPç‰¹å¾å‘é‡ç›¸ä¼¼åº¦ï¼‰
        try:
            scores["style_consistency"] = self._compute_style_similarity(ref_image, gen_image)
        except Exception as e:
            logger.warning(f"âš ï¸ è®¡ç®—é£Žæ ¼ä¸€è‡´æ€§å¤±è´¥: {e}")
            scores["style_consistency"] = 0.5

        # 2. å§¿æ€ç›¸ä¼¼åº¦ï¼ˆåŸºäºŽCLIPç©ºé—´ä¿¡æ¯ï¼‰
        try:
            scores["pose_similarity"] = self._estimate_pose_similarity(ref_image, gen_image)
        except Exception as e:
            logger.warning(f"âš ï¸ è®¡ç®—å§¿æ€ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            scores["pose_similarity"] = 0.5

        # 3. æž„å›¾ç›¸ä¼¼åº¦ï¼ˆè¾¹ç¼˜æ£€æµ‹å¯¹æ¯”ï¼‰
        try:
            scores["composition_match"] = self._compare_composition(ref_image, gen_image)
        except Exception as e:
            logger.warning(f"âš ï¸ è®¡ç®—æž„å›¾ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            scores["composition_match"] = 0.5

        # 4. è§’è‰²ä¸€è‡´æ€§ï¼ˆè‰²å½©ä¸Žçº¹ç†ï¼‰
        try:
            scores["character_consistency"] = self._compare_character_features(ref_image, gen_image)
        except Exception as e:
            logger.warning(f"âš ï¸ è®¡ç®—è§’è‰²ä¸€è‡´æ€§å¤±è´¥: {e}")
            scores["character_consistency"] = 0.5

        # 5. æ€»ä½“åŒ¹é…åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
        scores["overall_reference_match"] = (
            scores["style_consistency"] * 0.35
            + scores["pose_similarity"] * 0.25
            + scores["composition_match"] * 0.25
            + scores["character_consistency"] * 0.15
        )

        return scores

    def _compute_style_similarity(self, ref_image: Image.Image, gen_image: Image.Image) -> float:
        """è®¡ç®—é£Žæ ¼ä¸€è‡´æ€§ï¼ˆCLIPç‰¹å¾å‘é‡ç›¸ä¼¼åº¦ï¼‰"""
        with torch.no_grad():
            ref_inputs = self._processor(images=ref_image, return_tensors="pt").to(self.device)
            gen_inputs = self._processor(images=gen_image, return_tensors="pt").to(self.device)

            ref_features = self._ensure_feature_tensor(self._model.get_image_features(**ref_inputs))
            gen_features = self._ensure_feature_tensor(self._model.get_image_features(**gen_inputs))

            # L2 å½’ä¸€åŒ– (ä½¿ç”¨ torch.nn.functional.normalize)
            ref_features = torch.nn.functional.normalize(ref_features, p=2, dim=-1)
            gen_features = torch.nn.functional.normalize(gen_features, p=2, dim=-1)

            # ä½™å¼¦ç›¸ä¼¼åº¦ (èŒƒå›´: -1 åˆ° 1ï¼Œæˆ‘ä»¬æ˜ å°„åˆ° 0-1)
            similarity = (ref_features @ gen_features.T).item()
            return max(0.0, min(1.0, (similarity + 1) / 2))

    def _estimate_pose_similarity(self, ref_image: Image.Image, gen_image: Image.Image) -> float:
        """
        ä¼°è®¡å§¿æ€ç›¸ä¼¼åº¦
        ç®€åŒ–æ–¹æ¡ˆï¼šåŸºäºŽCLIPçš„ç©ºé—´åˆ†å¸ƒä¿¡æ¯
        """
        # å½“å‰ç®€åŒ–æ–¹æ¡ˆï¼šç”¨ CNN çš„å·ç§¯ç‰¹å¾å›¾å¯¹æ¯”
        # åŽç»­å¯å‡çº§ä¸º OpenPose/DWPose ç²¾ç¡®æå–éª¨éª¼å…³é”®ç‚¹

        with torch.no_grad():
            ref_inputs = self._processor(images=ref_image, return_tensors="pt").to(self.device)
            gen_inputs = self._processor(images=gen_image, return_tensors="pt").to(self.device)

            # ä½¿ç”¨è§†è§‰ç¼–ç å™¨çš„ä¸­å±‚ç‰¹å¾ï¼ˆå«ç©ºé—´ä¿¡æ¯ï¼‰
            ref_features = self._model.vision_model(**ref_inputs).last_hidden_state
            gen_features = self._model.vision_model(**gen_inputs).last_hidden_state

            # è®¡ç®—ç‰¹å¾å›¾çš„ç©ºé—´ç›¸ä¼¼æ€§
            ref_mean = ref_features.mean(dim=1)  # (1, 768)
            gen_mean = gen_features.mean(dim=1)

            # L2 å½’ä¸€åŒ– (ä½¿ç”¨ torch.nn.functional.normalize)
            ref_mean = torch.nn.functional.normalize(ref_mean, p=2, dim=-1)
            gen_mean = torch.nn.functional.normalize(gen_mean, p=2, dim=-1)

            similarity = (ref_mean @ gen_mean.T).item()
            return max(0.0, min(1.0, (similarity + 1) / 2))

    def _compare_composition(self, ref_image: Image.Image, gen_image: Image.Image) -> float:
        """è®¡ç®—æž„å›¾ç›¸ä¼¼åº¦ï¼ˆå¤šä¿¡å·èžåˆï¼šè¾¹ç¼˜å¸ƒå±€ + æ¢¯åº¦æ–¹å‘ + ä½Žåˆ†è¾¨çŽ‡ç»“æž„ï¼‰"""
        ref_array = cv2.cvtColor(np.array(ref_image), cv2.COLOR_RGB2BGR)
        gen_array = cv2.cvtColor(np.array(gen_image), cv2.COLOR_RGB2BGR)

        # è½¬ç°åº¦å¹¶å¯¹é½å°ºå¯¸
        ref_gray = cv2.cvtColor(ref_array, cv2.COLOR_BGR2GRAY)
        gen_gray = cv2.cvtColor(gen_array, cv2.COLOR_BGR2GRAY)

        if ref_gray.shape != gen_gray.shape:
            gen_gray = cv2.resize(gen_gray, (ref_gray.shape[1], ref_gray.shape[0]), interpolation=cv2.INTER_AREA)

        # ç»Ÿä¸€åˆ°è¾ƒå°å°ºå¯¸ï¼Œé™ä½Žå™ªå£°å¹¶çªå‡ºå¸ƒå±€ç»“æž„
        target_size = (256, 256)
        ref_small = cv2.resize(ref_gray, target_size, interpolation=cv2.INTER_AREA)
        gen_small = cv2.resize(gen_gray, target_size, interpolation=cv2.INTER_AREA)

        # Sobel æ¢¯åº¦ï¼ˆç»“æž„å¸ƒå±€ï¼‰
        ref_gx = cv2.Sobel(ref_small, cv2.CV_32F, 1, 0, ksize=3)
        ref_gy = cv2.Sobel(ref_small, cv2.CV_32F, 0, 1, ksize=3)
        gen_gx = cv2.Sobel(gen_small, cv2.CV_32F, 1, 0, ksize=3)
        gen_gy = cv2.Sobel(gen_small, cv2.CV_32F, 0, 1, ksize=3)

        ref_mag = cv2.magnitude(ref_gx, ref_gy)
        gen_mag = cv2.magnitude(gen_gx, gen_gy)

        # ä½Žåˆ†è¾¨çŽ‡å¸ƒå±€ç½‘æ ¼ï¼ˆè¾¹ç¼˜å¯†åº¦/ç»“æž„åˆ†å¸ƒï¼‰
        grid_size = (32, 32)
        ref_grid = cv2.resize(ref_mag, grid_size, interpolation=cv2.INTER_AREA)
        gen_grid = cv2.resize(gen_mag, grid_size, interpolation=cv2.INTER_AREA)

        # å½’ä¸€åŒ–åŽåšä½™å¼¦ç›¸ä¼¼åº¦
        ref_grid_vec = ref_grid.flatten().astype(np.float32)
        gen_grid_vec = gen_grid.flatten().astype(np.float32)
        ref_norm = np.linalg.norm(ref_grid_vec)
        gen_norm = np.linalg.norm(gen_grid_vec)
        if ref_norm == 0 or gen_norm == 0:
            grid_sim = 0.5
        else:
            grid_sim = float(np.dot(ref_grid_vec, gen_grid_vec) / (ref_norm * gen_norm))
            grid_sim = max(0.0, min(1.0, (grid_sim + 1.0) / 2.0))

        # æ¢¯åº¦æ–¹å‘ç›´æ–¹å›¾ï¼ˆç»“æž„æ–¹å‘ä¸€è‡´æ€§ï¼‰
        ref_angle = cv2.phase(ref_gx, ref_gy, angleInDegrees=True)
        gen_angle = cv2.phase(gen_gx, gen_gy, angleInDegrees=True)

        bins = 8
        bin_edges = np.linspace(0, 360, bins + 1)
        ref_hist = np.zeros(bins, dtype=np.float32)
        gen_hist = np.zeros(bins, dtype=np.float32)

        for i in range(bins):
            ref_mask = (ref_angle >= bin_edges[i]) & (ref_angle < bin_edges[i + 1])
            gen_mask = (gen_angle >= bin_edges[i]) & (gen_angle < bin_edges[i + 1])
            ref_hist[i] = ref_mag[ref_mask].sum()
            gen_hist[i] = gen_mag[gen_mask].sum()

        ref_hist_sum = ref_hist.sum()
        gen_hist_sum = gen_hist.sum()
        if ref_hist_sum == 0 or gen_hist_sum == 0:
            hist_sim = 0.5
        else:
            ref_hist /= ref_hist_sum
            gen_hist /= gen_hist_sum
            chi2 = 0.5 * np.sum(((ref_hist - gen_hist) ** 2) / (ref_hist + gen_hist + 1e-8))
            hist_sim = max(0.0, min(1.0, 1.0 - chi2))

        # è‡ªé€‚åº” Canny è¾¹ç¼˜ï¼ˆå¢žå¼ºå¯¹ç»“æž„è½®å»“çš„åˆ¤åˆ«ï¼‰
        ref_med = np.median(ref_small)
        gen_med = np.median(gen_small)
        ref_low = int(max(0, 0.66 * ref_med))
        ref_high = int(min(255, 1.33 * ref_med))
        gen_low = int(max(0, 0.66 * gen_med))
        gen_high = int(min(255, 1.33 * gen_med))

        ref_edges = cv2.Canny(ref_small, ref_low, ref_high)
        gen_edges = cv2.Canny(gen_small, gen_low, gen_high)

        intersection = np.logical_and(ref_edges, gen_edges).sum()
        union = np.logical_or(ref_edges, gen_edges).sum()
        if union == 0:
            edge_iou = 0.5
        else:
            edge_iou = intersection / union

        # ç»¼åˆè¯„åˆ†ï¼ˆå¯è°ƒæƒé‡ï¼‰
        composition_score = (grid_sim * 0.5) + (hist_sim * 0.3) + (edge_iou * 0.2)
        return float(max(0.0, min(1.0, composition_score)))

    def _compare_character_features(self, ref_image: Image.Image, gen_image: Image.Image) -> float:
        """è®¡ç®—è§’è‰²ä¸€è‡´æ€§ï¼ˆè‰²å½©ä¸Žçº¹ç†ï¼‰"""
        ref_array = np.array(ref_image)
        gen_array = np.array(gen_image)

        # è®¡ç®—ä¸»è‰²è°ƒçš„ç›¸ä¼¼åº¦
        ref_colors = self._extract_dominant_colors(ref_array)
        gen_colors = self._extract_dominant_colors(gen_array)

        color_similarity = self._compare_color_palettes(ref_colors, gen_colors)

        # è®¡ç®—ç›´æ–¹å›¾ç›¸ä¼¼åº¦ï¼ˆæ•´ä½“è‰²å½©åˆ†å¸ƒï¼‰
        hist_similarity = self._compare_histograms(ref_array, gen_array)

        return (color_similarity + hist_similarity) / 2

    def _extract_dominant_colors(self, image_array: np.ndarray, n_colors: int = 5) -> list:
        """æå–å›¾ç‰‡çš„ä¸»è¦é¢œè‰²"""
        # ç®€åŒ–ï¼šç›´æŽ¥ä½¿ç”¨å›¾ç‰‡çš„å¹³å‡è‰²å½©ç‰¹å¾
        pixels = image_array.reshape((-1, 3))
        pixels = np.float32(pixels)

        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        _, _, centers = cv2.kmeans(pixels, n_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

        return centers.astype(int).tolist()

    def _compare_color_palettes(self, colors1: list, colors2: list) -> float:
        """æ¯”è¾ƒä¸¤ä¸ªè‰²ç›˜"""
        # ç®€åŒ–æ–¹æ¡ˆï¼šè®¡ç®—æœ€è¿‘é‚»åŒ¹é…
        total_distance = 0
        for c1 in colors1:
            min_dist = min(np.linalg.norm(np.array(c1) - np.array(c2)) for c2 in colors2)
            total_distance += min_dist

        avg_distance = total_distance / len(colors1)
        # å½’ä¸€åŒ–åˆ° 0-1 (æœ€å¤§è·ç¦»çº¦255*sqrt(3) â‰ˆ 441)
        similarity = max(0.0, 1.0 - avg_distance / 441.0)
        return similarity

    def _compare_histograms(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """è®¡ç®—ç›´æ–¹å›¾ç›¸ä¼¼åº¦"""
        # è½¬HSVä»¥èŽ·å¾—æ›´å¥½çš„è‰²å½©æ„ŸçŸ¥
        img1_hsv = cv2.cvtColor(img1.astype(np.uint8), cv2.COLOR_RGB2HSV)
        img2_hsv = cv2.cvtColor(img2.astype(np.uint8), cv2.COLOR_RGB2HSV)

        hist1 = cv2.calcHist([img1_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])
        hist2 = cv2.calcHist([img2_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])

        hist1 = cv2.normalize(hist1, hist1).flatten()
        hist2 = cv2.normalize(hist2, hist2).flatten()

        similarity = cv2.compareHist(hist1, hist2, cv2.HISTCMP_BHATTACHARYYA)
        # è½¬æ¢ï¼šBhattacharyyaè·ç¦»è¶Šå°è¶Šå¥½ï¼ŒèŒƒå›´0-1
        return 1.0 - similarity

    def _default_scores(self) -> dict:
        """é»˜è®¤åˆ†æ•°ï¼ˆå‡ºé”™æ—¶è¿”å›žï¼‰"""
        return {
            "style_consistency": 0.5,
            "pose_similarity": 0.5,
            "composition_match": 0.5,
            "character_consistency": 0.5,
            "overall_reference_match": 0.5,
        }
